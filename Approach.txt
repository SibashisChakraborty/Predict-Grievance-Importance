Approach To Solve Problem:

Fit various classifiers to the data and see what yields the best outcome. The data has mostly categorical variables. We will follow the 'No Free Lunch' theorem, meaning that if no prior assumptions are made about the data there is no reason to prefer one model over any other.


Data Cleaning and Feature Engineering:

1. Remove any variable/feature that has greater than 70% missing values. Upon doing that we find one other feature with only 4 missing values and we simply remove those rows from our analysis.

2. Remove any feature that has Zero Variance, i.e has same values in all rows, hence model will not learn anything from the data.

3. Remove any redundant variables like country.alpha2 etc which has a one-to-one mapping with some other column that we are using in modeling, in the example above it will be country_name

4. Clean column names, replace (-,=,.) in column names with (_) to avoid confusion.

5. Create two features from Issues column, one is whether there was an issue with the complain, and number of issues, hence one is categorical(0/1) and other is numeric ranging between (0-27)

6. One-Hot-Encode all columns that have more than 2 int values(0,1,-1) and also any object column that we are considering as categorical.

7. Apply all these transformations on the submission test set.

8. Compute class weights for Full Training set, this will be used to fit on final model using full training data.


Modeling:

We used Decision Trees, BaggingClassifier ,Random Forest, Catboost, XGBoost, LightGBM to train our model, the best one came out to be XGBoost and hence only that is provided in the predictions file.

The best model from xgboost is selected after Hyperparameter tuning based on a Randomized Search over a parameter distribution.
