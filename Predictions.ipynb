{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import os\n",
    "from random import randint, randrange\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import mlxtend\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sibashis.chakrab\\\\Desktop\\\\Personal\\\\Hackerrank\\\\Machine Learning Challenge\\\\Submission_Hackerrank'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.getcwd(),'dataset')\n",
    "# output_folder = '/content/drive/MyDrive/ML Challenge Datasets/Output Folder'\n",
    "result_path = os.path.join(os.getcwd(),'Submissions')\n",
    "files = os.listdir(dataset_path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(dataset_path,files[1]),low_memory=False)\n",
    "test_data = pd.read_csv(os.path.join(dataset_path,files[0]),low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCombine All Issue columns into one single column with Issue as a binary variable and number of issues as another column\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "'''\n",
    "Combine All Issue columns into one single column with Issue as a binary variable and number of issues as another column\n",
    "'''\n",
    "issue_cols = [col for col in train if col.startswith('issue')]\n",
    "train['Issue_Count'] = train[issue_cols].apply(lambda x: x.count(), axis=1)\n",
    "train['Issue_Y_N'] = np.where(train['Issue_Count']== 0, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same on Test Data\n",
    "test_data['Issue_Count'] = test_data[issue_cols].apply(lambda x: x.count(), axis=1)\n",
    "test_data['Issue_Y_N'] = np.where(test_data['Issue_Count']== 0, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Treatment\n",
    "limit_per = len(train)*0.70\n",
    "train_clean = train.dropna(thresh=limit_per,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8874"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean = train_clean.dropna()\n",
    "len(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Columns with Zero Variance\n",
    "train_clean_col = train_clean\n",
    "for col in train_clean.columns:\n",
    "    #print('Cols {}'.format(col))\n",
    "    if len(train_clean[col].unique())==1:\n",
    "        #print('Removing feature: {}'.format(col))\n",
    "        train_clean_col = train_clean_col.drop(col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Other Redundant Features\n",
    "feat_remove = ['appno','country.alpha2','docname','ecli','itemid','originatingbody','sharepointid','parties.1','judgementdate',\\\n",
    "               'kpdate','respondentOrderEng','parties.0']\n",
    "train_cleaned = train_clean_col.drop(feat_remove,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to remove from Test set\n",
    "cols_to_remove = list(set(train.columns.tolist())-set(train_cleaned.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace (-,=,.) with (_) in column names\n",
    "train_cleaned.columns=train_cleaned.columns.str.replace('=','_')\n",
    "train_cleaned.columns=train_cleaned.columns.str.replace('-','_')\n",
    "train_cleaned.columns=train_cleaned.columns.str.replace('.','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find CCL columns replace (-1) with (2) and one-hot encode all columns that need to be encoded\n",
    "ccl_cols = [col for col in train_cleaned if col.startswith('ccl')]\n",
    "train_cleaned[ccl_cols] = train_cleaned[ccl_cols].replace([-1],[2])\n",
    "feature_encode_cols = ccl_cols + train_cleaned.select_dtypes(include=['object']).columns.tolist()+['typedescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "array_hot_encoded = ohe.fit_transform(train_cleaned[feature_encode_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ccl_article_1_0', 'ccl_article_1_1', 'ccl_article_1_2']\n",
      "['ccl_article_10_0', 'ccl_article_10_1', 'ccl_article_10_2']\n",
      "['ccl_article_11_0', 'ccl_article_11_1', 'ccl_article_11_2']\n",
      "['ccl_article_12_0', 'ccl_article_12_1', 'ccl_article_12_2']\n",
      "['ccl_article_13_0', 'ccl_article_13_1', 'ccl_article_13_2']\n",
      "['ccl_article_14_0', 'ccl_article_14_1', 'ccl_article_14_2']\n",
      "['ccl_article_17_0', 'ccl_article_17_2']\n",
      "['ccl_article_18_0', 'ccl_article_18_2']\n",
      "['ccl_article_2_0', 'ccl_article_2_1', 'ccl_article_2_2']\n",
      "['ccl_article_25_0', 'ccl_article_25_1', 'ccl_article_25_2']\n",
      "['ccl_article_3_0', 'ccl_article_3_1', 'ccl_article_3_2']\n",
      "['ccl_article_34_0', 'ccl_article_34_1', 'ccl_article_34_2']\n",
      "['ccl_article_38_0', 'ccl_article_38_1', 'ccl_article_38_2']\n",
      "['ccl_article_4_0', 'ccl_article_4_1', 'ccl_article_4_2']\n",
      "['ccl_article_46_0', 'ccl_article_46_1']\n",
      "['ccl_article_5_0', 'ccl_article_5_1', 'ccl_article_5_2']\n",
      "['ccl_article_6_0', 'ccl_article_6_1', 'ccl_article_6_2']\n",
      "['ccl_article_7_0', 'ccl_article_7_1', 'ccl_article_7_2']\n",
      "['ccl_article_8_0', 'ccl_article_8_1', 'ccl_article_8_2']\n",
      "['ccl_article_9_0', 'ccl_article_9_1', 'ccl_article_9_2']\n",
      "['ccl_article_p1_0', 'ccl_article_p1_1', 'ccl_article_p1_2']\n",
      "['ccl_article_p12_0', 'ccl_article_p12_1']\n",
      "['ccl_article_p4_0', 'ccl_article_p4_1', 'ccl_article_p4_2']\n",
      "['ccl_article_p6_0', 'ccl_article_p6_1']\n",
      "['ccl_article_p7_0', 'ccl_article_p7_1', 'ccl_article_p7_2']\n",
      "['country_name_Albania', 'country_name_Andorra', 'country_name_Armenia', 'country_name_Austria', 'country_name_Azerbaijan', 'country_name_Belgium', 'country_name_Bosnia and Herzegovina', 'country_name_Bulgaria', 'country_name_Croatia', 'country_name_Cyprus', 'country_name_Czechia', 'country_name_Denmark', 'country_name_Estonia', 'country_name_Finland', 'country_name_France', 'country_name_Georgia', 'country_name_Germany', 'country_name_Greece', 'country_name_Hungary', 'country_name_Iceland', 'country_name_Ireland', 'country_name_Italy', 'country_name_Latvia', 'country_name_Liechtenstein', 'country_name_Lithuania', 'country_name_Luxembourg', 'country_name_Malta', 'country_name_Moldova, Republic of', 'country_name_Montenegro', 'country_name_Netherlands', 'country_name_North Macedonia', 'country_name_Norway', 'country_name_Poland', 'country_name_Portugal', 'country_name_Romania', 'country_name_Russian Federation', 'country_name_San Marino', 'country_name_Serbia', 'country_name_Slovakia', 'country_name_Slovenia', 'country_name_Spain', 'country_name_Sweden', 'country_name_Switzerland', 'country_name_Turkey', 'country_name_Ukraine', 'country_name_United Kingdom']\n",
      "['doctypebranch_CHAMBER', 'doctypebranch_COMMITTEE', 'doctypebranch_GRANDCHAMBER']\n",
      "['originatingbody_name_Chamber', 'originatingbody_name_First Section', 'originatingbody_name_First Section Committee', 'originatingbody_name_Fith Section', 'originatingbody_name_Fith Section Committee', 'originatingbody_name_Fourth Section', 'originatingbody_name_Fourth Section Committee', 'originatingbody_name_Grand Chamber', 'originatingbody_name_Plenary', 'originatingbody_name_Second Section', 'originatingbody_name_Second Section Committee', 'originatingbody_name_Third Section', 'originatingbody_name_Third Section Committee']\n",
      "['respondent_0_ALB', 'respondent_0_AND', 'respondent_0_ARM', 'respondent_0_AUT', 'respondent_0_AZE', 'respondent_0_BEL', 'respondent_0_BGR', 'respondent_0_BIH', 'respondent_0_CHE', 'respondent_0_CYP', 'respondent_0_CZE', 'respondent_0_DEU', 'respondent_0_DNK', 'respondent_0_ESP', 'respondent_0_EST', 'respondent_0_FIN', 'respondent_0_FRA', 'respondent_0_GBR', 'respondent_0_GEO', 'respondent_0_GRC', 'respondent_0_HRV', 'respondent_0_HUN', 'respondent_0_IRL', 'respondent_0_ISL', 'respondent_0_ITA', 'respondent_0_LIE', 'respondent_0_LTU', 'respondent_0_LUX', 'respondent_0_LVA', 'respondent_0_MDA', 'respondent_0_MKD', 'respondent_0_MLT', 'respondent_0_MNE', 'respondent_0_NLD', 'respondent_0_NOR', 'respondent_0_POL', 'respondent_0_PRT', 'respondent_0_ROU', 'respondent_0_RUS', 'respondent_0_SMR', 'respondent_0_SRB', 'respondent_0_SVK', 'respondent_0_SVN', 'respondent_0_SWE', 'respondent_0_TUR', 'respondent_0_UKR']\n",
      "['typedescription_12', 'typedescription_14', 'typedescription_15', 'typedescription_18', 'typedescription_19']\n"
     ]
    }
   ],
   "source": [
    "# All columns after encoding\n",
    "column_names = []\n",
    "C=0\n",
    "for i in ohe.categories_:\n",
    "  print(list(feature_encode_cols[C]+'_'+pd.Series(i).apply(str)))\n",
    "  column_names.extend(list(feature_encode_cols[C]+'_'+pd.Series(i).apply(str)))\n",
    "  C=C+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hot_encoded = pd.DataFrame(array_hot_encoded, index=train_cleaned.index,columns=column_names,dtype=np.int64)\n",
    "data_others = train_cleaned.drop(feature_encode_cols,axis=1)\n",
    "train_final = pd.concat([data_hot_encoded,data_others],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same on test\n",
    "test_data_1 = test_data.drop(cols_to_remove,axis=1)\n",
    "test_data_1.columns=test_data_1.columns.str.replace('=','_')\n",
    "test_data_1.columns=test_data_1.columns.str.replace('-','_')\n",
    "test_data_1.columns=test_data_1.columns.str.replace('.','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1[ccl_cols] = test_data_1[ccl_cols].replace([-1],[2])\n",
    "array_hot_encoded_test = ohe.transform(test_data_1[feature_encode_cols])\n",
    "data_hot_encoded_test = pd.DataFrame(array_hot_encoded_test, index=test_data_1.index,columns=column_names,dtype=np.int64)\n",
    "data_others_test = test_data_1.drop(feature_encode_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = pd.concat([data_hot_encoded_test,data_others_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "413\n"
     ]
    }
   ],
   "source": [
    "print(len(train_final.columns.tolist()))\n",
    "print(len(test_final.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Train and Test\n",
    "\n",
    "## Define X, y\n",
    "X = train_final.drop(['importance'],axis=1).values\n",
    "y = train_final['importance'].values\n",
    "\n",
    "# Split Train Test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42,stratify=y)\n",
    "\n",
    "# Test Submission Matrix\n",
    "\n",
    "test_matrix = test_final.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Weight for Train Split and Full Training Set\n",
    "\n",
    "## Class Weight for Training Split\n",
    "from sklearn.utils import class_weight\n",
    "class_weights_train = list(class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(train_final['importance']),\n",
    "                                             y_train,classes=[1,2,3,4]))\n",
    "\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights_train[val-1]\n",
    "\n",
    "    \n",
    "## Class Weight for Full Training Set\n",
    "class_weights_ft = list(class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(train_final['importance']),\n",
    "                                             train_final['importance'],classes=[1,2,3,4]))\n",
    "\n",
    "w_array_ft = np.ones(y.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y):\n",
    "    w_array_ft[i] = class_weights_ft[val-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid XGBoost\n",
    "model = xgb.XGBClassifier(random_state=123,objective='multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate':[0.1,0.01,0.5,0.05],\n",
    "    'colsample_bytree':[0.2,0.4,0.6,0.8,1],\n",
    "    'subsample':np.linspace(0.4,1,num=5),\n",
    "    'max_depth':[10,15,20,25],\n",
    "    'n_estimators':[100,200,300,400,500],\n",
    "    'reg_lambda':np.linspace(1,2,num=5),\n",
    "    'gamma':np.linspace(0,0.5,num=5)\n",
    "}\n",
    "\n",
    "# scoring = {\n",
    "#     'Accuracy':make_scorer(accuracy_score)\n",
    "# }\n",
    "\n",
    "num_folds = 10\n",
    "kfold = StratifiedKFold(n_splits=num_folds,random_state=123,shuffle=True)\n",
    "n_iter = 50\n",
    "grid=RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions = param_grid,\n",
    "    random_state=123,\n",
    "    cv = kfold,\n",
    "    n_jobs=-1,\n",
    "    n_iter=n_iter,\n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training w/o sample weights\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = best_estimator.predict(X_test)\n",
    "pred_xgb_train = best_estimator.predict(X_train)\n",
    "print('Accuracy on test: {:.2f}'.format(accuracy_score(y_test,pred_xgb)))\n",
    "print('Accuracy on train: {:.2f}'.format(accuracy_score(y_train,pred_xgb_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.fit(X,y,sample_weight=w_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_xgb = best_estimator.predict(test_matrix)\n",
    "test_data['importance']= pred_test_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['appno','importance']].to_csv(os.path.join(result_path,'XGB_RandCV_cclbt02_md_25_est500_lr01_nl50_rlamb125_rand123_weights_Fin_Submission.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
